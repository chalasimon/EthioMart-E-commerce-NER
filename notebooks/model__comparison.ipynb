{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q seqeval\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXav55zltEXb",
        "outputId": "826ee9e5-013f-46f7-9391-3541be4d882a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Kduw4DcRsK7I"
      },
      "outputs": [],
      "source": [
        "# loading the libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "from datasets import Dataset, Features, Value, ClassLabel\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "from seqeval.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "import numpy as np\n",
        "import logging\n",
        "from itertools import chain\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset,concatenate_datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lE5Bu2xGxoiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = [\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Kifiya Challenge/week4/data/labelled_data.conll',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Kifiya Challenge/week4/data/amharic_ner.conll',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Kifiya Challenge/week4/data/ner_auto_labels.conll',\n",
        "]"
      ],
      "metadata": {
        "id": "tc81GqV6sStR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the models\n",
        "MODEL_NAME_1 = \"/content/drive/MyDrive/week4/model/xlm-roberta-model\"\n",
        "MODEL_NAME_2 = \"/content/drive/MyDrive/week4/model/bert-tiny-amh-model\""
      ],
      "metadata": {
        "id": "sekIA8uet0kG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_1)"
      ],
      "metadata": {
        "id": "3EgLFm4wwC7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_conll_file(filepath):\n",
        "    tokens, tags = [], []\n",
        "    sentence, label_seq = [], []\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                if sentence:\n",
        "                    tokens.append(sentence)\n",
        "                    tags.append(label_seq)\n",
        "                    sentence, label_seq = [], []\n",
        "            else:\n",
        "                if len(line.split()) == 2:\n",
        "                    token, label = line.split()\n",
        "                    sentence.append(token)\n",
        "                    label_seq.append(label)\n",
        "    # Add last sentence\n",
        "    if sentence:\n",
        "        tokens.append(sentence)\n",
        "        tags.append(label_seq)\n",
        "\n",
        "    # ✅ Return DataFrame\n",
        "    return pd.DataFrame({'tokens': tokens, 'ner_tags': tags})"
      ],
      "metadata": {
        "id": "E7p4G70MwM2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load each file\n",
        "dataset_list = []\n",
        "for path in file_path:\n",
        "    try:\n",
        "        df = parse_conll_file(path)\n",
        "        if not df.empty:\n",
        "            dataset = Dataset.from_pandas(df)\n",
        "            dataset_list.append(dataset)\n",
        "            print(f\" Loaded {len(df)} sentences from: {path}\")\n",
        "        else:\n",
        "            print(f\"File is empty: {path}\")\n",
        "    except Exception as e:\n",
        "        print(f\" Error reading {path}: {e}\")\n",
        "\n",
        "# Combine datasets\n",
        "if dataset_list:\n",
        "    c_dataset = concatenate_datasets(dataset_list)\n",
        "    print(\" Combined dataset with total samples:\", len(c_dataset))\n",
        "else:\n",
        "    print(\"No datasets were successfully loaded.\")"
      ],
      "metadata": {
        "id": "pD3eAsDTwQTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten all tags into a single list\n",
        "all_tags = list(chain.from_iterable(c_dataset['ner_tags']))\n",
        "unique_tags = sorted(set(all_tags))  # -> ENTITY_LABELS\n",
        "label2id = {label: idx for idx, label in enumerate(unique_tags)}\n",
        "id2label = {idx: label for label, idx in label2id.items()}\n",
        "print(\"Label2ID mapping:\", label2id)\n",
        "\n",
        "#  Encode ner_tags to IDs\n",
        "def encode_labels(example):\n",
        "    return {\"ner_tags\": [label2id[tag] for tag in example[\"ner_tags\"]]}\n",
        "\n",
        "# Apply label encoding to dataset\n",
        "eval_dataset = c_dataset.map(encode_labels)"
      ],
      "metadata": {
        "id": "yQ7iwmNFyDu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples['tokens'], truncation=True, is_split_into_words=True)\n",
        "    labels = []\n",
        "\n",
        "    for i, label in enumerate(examples['ner_tags']):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        label_ids = []\n",
        "        previous_word_idx = None\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ],
      "metadata": {
        "id": "uhNQOn4KxPsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset = eval_dataset.map(tokenize_and_align_labels, batched=True)"
      ],
      "metadata": {
        "id": "Gi9Et6pSxRfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    preds = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_labels = [\n",
        "        [id2label[label] for label in example if label != -100]\n",
        "        for example in labels\n",
        "    ]\n",
        "    true_preds = [\n",
        "        [id2label[pred] for pred, label in zip(pred_seq, label_seq) if label != -100]\n",
        "        for pred_seq, label_seq in zip(preds, labels)\n",
        "    ]\n",
        "\n",
        "    return {\n",
        "        \"f1\": f1_score(true_labels, true_preds),\n",
        "        \"precision\": precision_score(true_labels, true_preds),\n",
        "        \"recall\": recall_score(true_labels, true_preds),\n",
        "        \"accuracy\": accuracy_score(true_labels, true_preds)\n",
        "    }\n"
      ],
      "metadata": {
        "id": "vOr82snTyVyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model_path in [MODEL_NAME_1, MODEL_NAME_2]:\n",
        "    print(f\"\\ Evaluating model: {model_path}\")\n",
        "    model = AutoModelForTokenClassification.from_pretrained(\n",
        "        model_path, num_labels=NUM_LABELS, id2label=id2label, label2id=label2id\n",
        "    )\n",
        "    trainer = Trainer(model=model, tokenizer=tokenizer, compute_metrics=compute_metrics)\n",
        "    results = trainer.evaluate(eval_dataset)\n",
        "    print(f\"Results for {model_path}: {results}\")"
      ],
      "metadata": {
        "id": "2MgAUL9cyaPk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}